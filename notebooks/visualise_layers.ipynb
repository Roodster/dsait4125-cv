{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.networks import MAGANet  # Replace with your actual model import\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MAGANet.__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMAGANet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Adjust parameters as per your model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../outputs/magan_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mTypeError\u001b[0m: MAGANet.__init__() got an unexpected keyword argument 'in_channels'"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MAGANet(in_channels=1, latent_dim=10).to(device)  # Adjust parameters as per your model\n",
    "model.load_state_dict(torch.load(\"../../outputs/magan_model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store intermediate outputs\n",
    "intermediate_outputs = {}\n",
    "\n",
    "# Hook function to capture outputs\n",
    "def get_activation(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "# Attach hooks to the desired layers\n",
    "# Affine layer\n",
    "model.decoder.affine.register_forward_hook(get_activation('affine'))\n",
    "\n",
    "# Flow modules and their subcomponents\n",
    "for i, flow_module in enumerate(model.decoder.flow_modules):\n",
    "    flow_module.register_forward_hook(get_activation(f'flow_module_{i}'))\n",
    "    for j, flow_step in enumerate(flow_module.flow_steps):\n",
    "        flow_step.register_forward_hook(get_activation(f'flow_module_{i}_step_{j}'))\n",
    "        flow_step.act_norm.register_forward_hook(get_activation(f'flow_module_{i}_step_{j}_actnorm'))\n",
    "        flow_step.inv_conv.register_forward_hook(get_activation(f'flow_module_{i}_step_{j}_invconv'))\n",
    "        flow_step.coupling.register_forward_hook(get_activation(f'flow_module_{i}_step_{j}_coupling'))\n",
    "\n",
    "# Encoder convolutional layers\n",
    "for i, layer in enumerate(model.encoder.conv):\n",
    "    layer.register_forward_hook(get_activation(f'encoder_conv_{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_latents(model, x1, x2):\n",
    "    with torch.no_grad():\n",
    "        z, mu1, logvar1, mu2, logvar2 = model.encoder(x1, x2)\n",
    "        z1 = model.encoder.sample_z(mu1, logvar1)  # Sample z1\n",
    "        z2 = model.encoder.sample_z(mu2, logvar2)  # Sample z2\n",
    "        return z, z1, z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_output(output, name):\n",
    "    if output.dim() == 4:  # [B, C, H, W]\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        num_channels = min(5, output.size(1))  # Show up to 5 channels\n",
    "        for i in range(num_channels):\n",
    "            plt.subplot(1, num_channels, i + 1)\n",
    "            plt.imshow(output[0, i].cpu().detach().numpy(), cmap='gray')\n",
    "            plt.title(f'{name} Ch{i}')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Cannot visualize {name}: unexpected shape {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent(z, name):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    z = z.cpu().detach().numpy()\n",
    "    if z.ndim == 1:  # Single vector\n",
    "        plt.bar(range(len(z)), z)\n",
    "    elif z.ndim == 2:  # Batch of vectors\n",
    "        plt.hist(z.flatten(), bins=30)\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Dimension' if z.ndim == 1 else 'Value')\n",
    "    plt.ylabel('Value' if z.ndim == 1 else 'Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_outputs(model, x1, x2):\n",
    "    # Clear previous outputs\n",
    "    intermediate_outputs.clear()\n",
    "\n",
    "    # Run the forward pass\n",
    "    with torch.no_grad():\n",
    "        generated_x2 = model(x1, x2)  # Standard forward pass\n",
    "        z, z1, z2 = capture_latents(model, x1, x2)  # Capture latent vectors\n",
    "\n",
    "    # Visualize encoder convolutional outputs\n",
    "    print(\"Encoder Convolutional Outputs:\")\n",
    "    for key in sorted([k for k in intermediate_outputs.keys() if k.startswith('encoder_conv_')]):\n",
    "        visualize_image_output(intermediate_outputs[key], key)\n",
    "\n",
    "    # Visualize latent vectors\n",
    "    print(\"Latent Vectors:\")\n",
    "    visualize_latent(z1, 'z1')\n",
    "    visualize_latent(z2, 'z2')\n",
    "    visualize_latent(z, 'z (z2 - z1)')\n",
    "\n",
    "    # Visualize decoder outputs\n",
    "    print(\"Decoder Outputs:\")\n",
    "    # Affine layer\n",
    "    visualize_image_output(intermediate_outputs.get('affine'), 'affine')\n",
    "\n",
    "    # Flow modules and their subcomponents\n",
    "    for key in sorted(intermediate_outputs.keys()):\n",
    "        if key.startswith('flow_module_'):\n",
    "            if 'step' not in key and 'actnorm' not in key and 'invconv' not in key and 'coupling' not in key:\n",
    "                visualize_image_output(intermediate_outputs[key], key)  # Flow module output\n",
    "            elif 'step' in key and 'actnorm' not in key and 'invconv' not in key and 'coupling' not in key:\n",
    "                visualize_image_output(intermediate_outputs[key], key)  # Flow step output\n",
    "            elif 'actnorm' in key:\n",
    "                visualize_image_output(intermediate_outputs[key], key)  # ActNorm output\n",
    "            elif 'invconv' in key:\n",
    "                visualize_image_output(intermediate_outputs[key], key)  # Invertible1x1Conv output\n",
    "            elif 'coupling' in key:\n",
    "                visualize_image_output(intermediate_outputs[key], key)  # AdditiveCoupling output\n",
    "\n",
    "    # Optionally visualize the final generated image\n",
    "    print(\"Generated Output:\")\n",
    "    visualize_image_output(generated_x2, 'generated_x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with dummy data\n",
    "x1 = torch.randn(1, 1, 64, 64).to(device)  # [B, C, H, W]\n",
    "x2 = torch.randn(1, 1, 64, 64).to(device)\n",
    "visualize_model_outputs(model, x1, x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
